{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log, sqrt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train_enc.tsv'\n",
    "DEV_PATH = 'data/dev_enc.tsv'\n",
    "TEST_PATH = 'data/test_enc_unlabeled.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH, sep='\\t', names=['label', 'content'])\n",
    "train_result =list(train_df.to_records(index=False))\n",
    "num_train = len(train_result)\n",
    "\n",
    "val_df = pd.read_csv(DEV_PATH, sep='\\t', names=['label', 'content'])\n",
    "val_result =list(val_df.to_records(index=False))\n",
    "num_val = len(val_result)\n",
    "\n",
    "test_df = pd.read_csv(TEST_PATH, sep='\\t', names=['content'])\n",
    "test_result =list(test_df.to_records(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding and Transformer Model with BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    Math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_class, embed, nhead, nhid, nlayers, max_seq=15, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        # self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.ninp = embed.shape[1]\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(self.ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(self.ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = torch.nn.Embedding.from_pretrained(embed, freeze=False)\n",
    "        self.decoder = nn.Linear(self.ninp*max_seq, n_class)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=False):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "        src = self.encoder(src.long()) * sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)   # [batch_size, seq_length, embed_dim]\n",
    "        word_att = output\n",
    "        scores = self.decoder(output.view(output.size(0), -1))        # (batch_size, seq_length * embed_dim) -> (batch_size, n_classes)\n",
    "        # return F.log_softmax(output, dim=-1)\n",
    "        return scores, word_att  # (batch_size, n_classes) & (batch_size, seq_length, emb_dim)\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embed, hidden_dim1,  n_class, dropout=0):\n",
    "        super(RNN, self).__init__()\n",
    "        vocab_dim, embed_dim = embed.shape\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embed, freeze=False)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim1, batch_first=True, num_layers=2, dropout=dropout)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim1, batch_first=True, num_layers=1)\n",
    "        self.fc1 = nn.Linear(hidden_dim1, n_class)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        out, ht = self.rnn(x)\n",
    "        out = self.drop(out[:, -1, :])        \n",
    "        out = self.fc1(out)\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn1(batch): # Train\n",
    "    text_list, label_list = [], []\n",
    "    for (label_, text_) in batch:\n",
    "        label_list.append(label_)\n",
    "        process_text = torch.tensor(text_pipeline(text_), dtype=torch.int64)\n",
    "        text_list.append(process_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "def collate_fn2(batch):# Test\n",
    "    text_list= []\n",
    "    for tup in batch:\n",
    "        text_ = tup[0]\n",
    "        process_text = torch.tensor(text_pipeline(text_), dtype=torch.int64)\n",
    "        text_list.append(process_text)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return text_list.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vocab from pre-trained embedding vocab\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "l = []\n",
    "for title in train_df['content']:\n",
    "    tokens = tokenizer.tokenize(title)\n",
    "    counter.update(tokens)\n",
    "    l.append(tokens)\n",
    "vocab = Vocab(counter, min_freq=2, vectors='glove.6B.300d')\n",
    "embed = vocab.vectors\n",
    "\n",
    "# hyper-parameter\n",
    "model = RNN(embed, hidden_dim1=64, n_class=5, dropout = 0.5)\n",
    "lr = 0.001\n",
    "epoch = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=[3,6,10])\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer.tokenize(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch up training data \n",
    "train_loader = torch.utils.data.DataLoader(train_result, batch_size=4, shuffle=True, collate_fn=collate_fn1)\n",
    "val_loader = torch.utils.data.DataLoader(val_result, batch_size=4, shuffle=False, collate_fn=collate_fn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/4055 [00:00<02:55, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== epoch 1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [01:59<00:00, 34.00it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 526.01it/s]\n",
      "  0%|          | 4/4055 [00:00<02:00, 33.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.5602561339203145\n",
      "training acc: 69.92602%\n",
      "validation loss: 0.3787185097116926\n",
      "validation acc: 83.86778%\n",
      "train and test cost 120.22848081588745 seconds\n",
      "==== epoch 2 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:12<00:00, 30.55it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 525.74it/s]\n",
      "  0%|          | 4/4055 [00:00<02:07, 31.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.26362572610010787\n",
      "training acc: 90.25277%\n",
      "validation loss: 0.33943113067446373\n",
      "validation acc: 86.28515%\n",
      "train and test cost 135.31686305999756 seconds\n",
      "==== epoch 3 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:06<00:00, 32.04it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 523.13it/s]\n",
      "  0%|          | 4/4055 [00:00<02:04, 32.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.16659718241850338\n",
      "training acc: 94.18619%\n",
      "validation loss: 0.3473000912036181\n",
      "validation acc: 88.01183999999999%\n",
      "train and test cost 129.2291579246521 seconds\n",
      "==== epoch 4 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:17<00:00, 29.42it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 499.00it/s]\n",
      "  0%|          | 4/4055 [00:00<02:04, 32.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.09028265373333462\n",
      "training acc: 96.9852%\n",
      "validation loss: 0.4106502796301005\n",
      "validation acc: 88.20918%\n",
      "train and test cost 140.55921506881714 seconds\n",
      "==== epoch 5 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:37<00:00, 25.77it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 446.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.07987382655961005\n",
      "training acc: 97.21948%\n",
      "validation loss: 0.42200122359236314\n",
      "validation acc: 88.40651000000001%\n",
      "train and test cost 160.27647590637207 seconds\n",
      "==== epoch 6 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:16<00:00, 29.77it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 518.67it/s]\n",
      "  0%|          | 4/4055 [00:00<02:03, 32.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.07059125229991908\n",
      "training acc: 97.43527%\n",
      "validation loss: 0.43676971495739336\n",
      "validation acc: 88.55451%\n",
      "train and test cost 139.18735790252686 seconds\n",
      "==== epoch 7 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:10<00:00, 30.98it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 511.29it/s]\n",
      "  0%|          | 3/4055 [00:00<02:16, 29.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.06057963153730926\n",
      "training acc: 97.78052000000001%\n",
      "validation loss: 0.4424288597332655\n",
      "validation acc: 88.55451%\n",
      "train and test cost 133.557137966156 seconds\n",
      "==== epoch 8 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:19<00:00, 29.09it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 491.34it/s]\n",
      "  0%|          | 4/4055 [00:00<02:08, 31.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.06041657886434572\n",
      "training acc: 97.78052000000001%\n",
      "validation loss: 0.447921060716377\n",
      "validation acc: 88.60385%\n",
      "train and test cost 142.33232283592224 seconds\n",
      "==== epoch 9 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:17<00:00, 29.44it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 492.93it/s]\n",
      "  0%|          | 3/4055 [00:00<02:21, 28.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.05861143965903399\n",
      "training acc: 97.78052000000001%\n",
      "validation loss: 0.45335216635077663\n",
      "validation acc: 88.65317999999999%\n",
      "train and test cost 140.59218788146973 seconds\n",
      "==== epoch 10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:14<00:00, 30.23it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 490.08it/s]\n",
      "  0%|          | 4/4055 [00:00<02:04, 32.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.05694825799191777\n",
      "training acc: 97.84834000000001%\n",
      "validation loss: 0.45840749044625245\n",
      "validation acc: 88.60385%\n",
      "train and test cost 137.03507614135742 seconds\n",
      "==== epoch 11 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:14<00:00, 30.19it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 519.97it/s]\n",
      "  0%|          | 3/4055 [00:00<02:19, 29.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.056858556632195974\n",
      "training acc: 97.89149%\n",
      "validation loss: 0.45886931710929796\n",
      "validation acc: 88.65317999999999%\n",
      "train and test cost 137.09126591682434 seconds\n",
      "==== epoch 12 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:13<00:00, 30.33it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 497.40it/s]\n",
      "  0%|          | 3/4055 [00:00<03:21, 20.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.05649235698356581\n",
      "training acc: 97.9963%\n",
      "validation loss: 0.4593448676536304\n",
      "validation acc: 88.60385%\n",
      "train and test cost 136.50937414169312 seconds\n",
      "==== epoch 13 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:19<00:00, 29.06it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 511.02it/s]\n",
      "  0%|          | 3/4055 [00:00<02:19, 29.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.05568555650816893\n",
      "training acc: 97.89766%\n",
      "validation loss: 0.4597881249422152\n",
      "validation acc: 88.60385%\n",
      "train and test cost 142.40285897254944 seconds\n",
      "==== epoch 14 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:11<00:00, 30.86it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 525.83it/s]\n",
      "  0%|          | 3/4055 [00:00<02:16, 29.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.057272944703202065\n",
      "training acc: 97.92232%\n",
      "validation loss: 0.46013498400325137\n",
      "validation acc: 88.65317999999999%\n",
      "train and test cost 134.20865511894226 seconds\n",
      "==== epoch 15 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:21<00:00, 28.64it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 510.39it/s]\n",
      "  0%|          | 3/4055 [00:00<02:18, 29.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.058204840908215164\n",
      "training acc: 97.82366999999999%\n",
      "validation loss: 0.46054775380994206\n",
      "validation acc: 88.65317999999999%\n",
      "train and test cost 144.2464029788971 seconds\n",
      "==== epoch 16 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:18<00:00, 29.34it/s]\n",
      "100%|██████████| 507/507 [00:00<00:00, 518.75it/s]\n",
      "  0%|          | 4/4055 [00:00<02:02, 33.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.057144172564975726\n",
      "training acc: 97.873%\n",
      "validation loss: 0.46094132222132334\n",
      "validation acc: 88.65317999999999%\n",
      "train and test cost 141.12920808792114 seconds\n",
      "==== epoch 17 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:12<00:00, 30.51it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 494.28it/s]\n",
      "  0%|          | 3/4055 [00:00<02:38, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.057279842202672245\n",
      "training acc: 97.836%\n",
      "validation loss: 0.4613965277135725\n",
      "validation acc: 88.65317999999999%\n",
      "train and test cost 135.63008213043213 seconds\n",
      "==== epoch 18 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:20<00:00, 28.76it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 489.15it/s]\n",
      "  0%|          | 4/4055 [00:00<02:13, 30.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.05668388018625732\n",
      "training acc: 97.99014%\n",
      "validation loss: 0.46173396665433925\n",
      "validation acc: 88.65317999999999%\n",
      "train and test cost 143.87544798851013 seconds\n",
      "==== epoch 19 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [02:26<00:00, 27.77it/s]\n",
      "100%|██████████| 507/507 [00:01<00:00, 453.64it/s]\n",
      "  0%|          | 3/4055 [00:00<02:17, 29.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.056708689359201606\n",
      "training acc: 97.89766%\n",
      "validation loss: 0.46220211352587215\n",
      "validation acc: 88.70251999999999%\n",
      "train and test cost 149.07496786117554 seconds\n",
      "==== epoch 20 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4055/4055 [04:32<00:00, 14.89it/s]\n",
      "100%|██████████| 507/507 [00:03<00:00, 165.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "training loss: 0.05569202503058531\n",
      "training acc: 97.92232%\n",
      "validation loss: 0.4625662779196715\n",
      "validation acc: 88.70251999999999%\n",
      "train and test cost 277.3453950881958 seconds\n",
      "\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_val_loss = []\n",
    "epoch_val_acc = []\n",
    "\n",
    "for epoch_idx in range(1, epoch+1):\n",
    "    # print(scheduler.get_last_lr())\n",
    "    print('==== epoch {} ===='.format(epoch_idx))\n",
    "    epoch_start = time.time()\n",
    "    accurate_count = 0\n",
    "    batch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (label, text) in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        out = model(text)\n",
    "        \n",
    "        # cal loss\n",
    "        loss = loss_fn(out, label)\n",
    "        batch_loss += loss\n",
    "        \n",
    "        # Backpropagation (BP)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # print('predicted: ', predicted, 'label: ', label)\n",
    "        tmp = np.count_nonzero((predicted==label).cpu().detach().numpy())\n",
    "        accurate_count += tmp\n",
    "        # End of Train    \n",
    "    epoch_train_loss.append(batch_loss.detach().numpy()/(batch_idx+1))\n",
    "    epoch_train_acc.append(round(accurate_count/num_train, 7)*100)    \n",
    "\n",
    "    # validation\n",
    "    val_loss = 0\n",
    "    val_acc_count = 0\n",
    "    model.eval()        \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (label, text) in enumerate(tqdm(val_loader)):\n",
    "            # forward\n",
    "            out = model(text)\n",
    "            \n",
    "            # cal loss\n",
    "            loss = loss_fn(out, label)\n",
    "            val_loss += loss\n",
    "                        \n",
    "            # calculate accuracy\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            valtmp = np.count_nonzero((predicted==label).cpu().detach().numpy())\n",
    "            val_acc_count += valtmp    \n",
    "            \n",
    "    scheduler.step()\n",
    "    epoch_val_loss.append(val_loss.detach().numpy()/(batch_idx+1))\n",
    "    epoch_val_acc.append(round(val_acc_count/num_val, 7)*100)       \n",
    "    print('================================')\n",
    "    print('training loss: {}'.format(epoch_train_loss[-1]))\n",
    "    print('training acc: {}%'.format(epoch_train_acc[-1]))\n",
    "    print('validation loss: {}'.format((epoch_val_loss[-1])))\n",
    "    print('validation acc: {}%'.format(epoch_val_acc[-1]))\n",
    "    \n",
    "    per_epoch_time = time.time() - epoch_start\n",
    "    print('train and test cost {} seconds'.format(per_epoch_time))\n",
    "print('\\nFinished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2028/2028 [00:02<00:00, 883.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Batch up test data\n",
    "test_loader = torch.utils.data.DataLoader(test_result, batch_size=1, shuffle=False, collate_fn=collate_fn2)\n",
    "\n",
    "test_output = [] \n",
    "with torch.no_grad():\n",
    "    for input_tensor in tqdm(test_loader):\n",
    "        # forward\n",
    "        out = model(input_tensor)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # print('predicted: ', predicted)\n",
    "        test_output.append(int(predicted.cpu().detach().numpy()))\n",
    "\n",
    "# print(\"Test Output: \", test_output)\n",
    "results = test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "# those results are in the list called 'results'\n",
    "assert (len(results) == 2028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "results = [int(x) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "with open('upload_predictions_transformer_pytorch.txt', 'w', encoding = 'utf-8') as fp:\n",
    "    for x in results:\n",
    "        fp.write(str(x) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCES:\n",
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "# https://atheros.ai/blog/text-classification-with-transformers-in-tensorflow-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
